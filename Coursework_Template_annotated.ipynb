{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install swig\n",
    "!pip install rldurham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable CUDA (better on Colab/NCC: choose an environment without GPU)\n",
    "import torch\n",
    "import rldurham as rld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "**Reinforcement learning agent**\n",
    "\n",
    "Replace this with your own agent. Also see the SAC implementationi below for a starting point. This is a minimum random agent scaffold and a soft actor-critic style implementation. \n",
    "\n",
    "## This is a minimal placeholder agent, it is a template to show how the training loop is wired.\n",
    "\n",
    "`rld.env_info(env)` returns information like whether action/object spaces are discrete, what the action dimention size is, what the obsetvation size is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Agent, self).__init__()\n",
    "        self.discrete_act, self.discrete_obs, self.act_dim, self.obs_dim = rld.env_info(env)\n",
    "\n",
    "    def sample_action(self, s):\n",
    "        return torch.rand(self.act_dim) * 2 - 1 # unifrom random in [-1, 1]\n",
    "\n",
    "    def put_data(self, action, observation, reward):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**\n",
    "\n",
    "This is where we create an enviroment, for example, `rldurham/Walker`. This is then wrapped with a `rld.Recorder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"lspf78-agent-video\",  # prefix for videos (replace lspf78 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image (reset just to render image)\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect episodes and train the agent**\n",
    "\n",
    "This is the main template loop, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rDl6ViIDlVOk",
    "outputId": "731e4ce7-c98d-4bde-8a2c-fbdf1410e24f"
   },
   "outputs": [],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# initialise agent\n",
    "agent = Agent(env)\n",
    "max_episodes = 10\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "# switch video recording off (only switch on every x episodes as this is slow)\n",
    "env.video = False\n",
    "\n",
    "# training procedure\n",
    "for episode in range(max_episodes):\n",
    "    \n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    env.info = True                 # usually tracking every episode is fine\n",
    "    env.video = episode % 100 == 0  # record videos every 100 episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # select the agent action\n",
    "        action = agent.sample_action(observation)\n",
    "\n",
    "        # take action in the environment\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # remember\n",
    "        agent.put_data(action, observation, reward)\n",
    "\n",
    "        # check whether done\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # train the agent after each episode\n",
    "    agent.train()\n",
    "            \n",
    "    # track and plot statistics\n",
    "    tracker.track(info)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file (for coursework)\n",
    "env.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")  # replace xxxx00 with your username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (designed for the orignal BipedalWalker environment, but should also work fine for rldurham/Walker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500): # run for 500 steps\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Soft-Actor-Critic (SAC) implementation (adapted from https://github.com/seungeunrho/minimalRL/blob/master/sac.py) that does not perform very well, but is a decent starting point for the coursework. It can be outperformed by simpler methods (e.g. TD3) if they are well tuned and you are free to use any other method. Overall, the task is to make use of various ideas and principles that were covered in the lecture and/or that you find in papers to achieve the best possible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import collections, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        s_lst = np.array(s_lst)\n",
    "        a_lst = np.array(a_lst)\n",
    "        r_lst = np.array(r_lst)\n",
    "        s_prime_lst = np.array(s_prime_lst)\n",
    "        done_mask_lst = np.array(done_mask_lst)\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.float), \\\n",
    "                torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, learning_rate, act_dim, obs_dim, init_alpha, lr_alpha):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        latent_dim = max(128, 2 * obs_dim)\n",
    "        self.fc1 = nn.Linear(obs_dim, latent_dim)\n",
    "        self.fc_mu = nn.Linear(latent_dim, act_dim)\n",
    "        self.fc_std = nn.Linear(latent_dim, act_dim)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.log_alpha = torch.tensor(np.log(init_alpha))\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=lr_alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        real_action = torch.tanh(action)\n",
    "        real_log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)\n",
    "        return real_action, real_log_prob\n",
    "\n",
    "    def train_net(self, q1, q2, mini_batch, target_entropy):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob\n",
    "\n",
    "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "\n",
    "        loss = -min_q - entropy # for gradient ascent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, learning_rate, act_dim, obs_dim):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc_s = nn.Linear(obs_dim, 64)\n",
    "        self.fc_a = nn.Linear(act_dim,64)\n",
    "        self.fc_cat = nn.Linear(128,32)\n",
    "        self.fc_out = nn.Linear(32, act_dim)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        h1 = F.relu(self.fc_s(x))\n",
    "        h2 = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([h1, h2], dim=1)\n",
    "        q = F.relu(self.fc_cat(cat))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "    def train_net(self, target, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        loss = F.smooth_l1_loss(self.forward(s, a), target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, net_target, tau):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 lr_pi=0.0005,\n",
    "                 lr_q=0.001,\n",
    "                 init_alpha=0.01,\n",
    "                 buffer_limit=50_000,\n",
    "                 gamma=0.98,\n",
    "                 batch_size=128,\n",
    "                 tau=0.01,  # for target network soft update\n",
    "                 target_entropy=-1.0,  # for automated alpha update\n",
    "                 train_steps=100,\n",
    "                 lr_alpha=0.001,  # for automated alpha update\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.target_entropy = target_entropy\n",
    "        self.train_steps = train_steps\n",
    "\n",
    "        discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_limit=buffer_limit)\n",
    "        self.q1 = QNet(lr_q, act_dim, obs_dim)\n",
    "        self.q2 = QNet(lr_q, act_dim, obs_dim)\n",
    "        self.q1_target = QNet(lr_q, act_dim, obs_dim)\n",
    "        self.q2_target = QNet(lr_q, act_dim, obs_dim)\n",
    "        self.pi = PolicyNet(lr_pi, act_dim, obs_dim, init_alpha=init_alpha, lr_alpha=lr_alpha)\n",
    "\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "    def action(self, s):\n",
    "        a, log_prob = self.pi(torch.from_numpy(s).float())\n",
    "        a = a.detach().numpy()\n",
    "        a *= 2\n",
    "        # assert np.all(a >= -0.4) and np.all(a <= 0.4)\n",
    "        return a\n",
    "\n",
    "    def put(self, s, a, r, s_prime, done):\n",
    "        self.memory.put((s, a, r / 10.0, s_prime, done))\n",
    "\n",
    "    def train(self):\n",
    "        if self.memory.size() > 1000:\n",
    "            for i in range(self.train_steps):\n",
    "                mini_batch = self.memory.sample(self.batch_size)\n",
    "                td_target = self.calc_target(mini_batch)\n",
    "                self.q1.train_net(td_target, mini_batch)\n",
    "                self.q2.train_net(td_target, mini_batch)\n",
    "                self.pi.train_net(self.q1, self.q2, mini_batch, target_entropy=self.target_entropy)\n",
    "                self.q1.soft_update(self.q1_target, tau=self.tau)\n",
    "                self.q2.soft_update(self.q2_target, tau=self.tau)\n",
    "\n",
    "    def calc_target(self, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_prime, log_prob = self.pi(s_prime)\n",
    "            entropy = -self.pi.log_alpha.exp() * log_prob\n",
    "            q1_val = self.q1(s_prime, a_prime)\n",
    "            q2_val = self.q2(s_prime, a_prime)\n",
    "            q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "            min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "            target = r + self.gamma * done * (min_q + entropy)\n",
    "\n",
    "        return target\n",
    "\n",
    "    def episode(self):\n",
    "        done = False\n",
    "        s, _ = self.env.reset()\n",
    "        info = {}\n",
    "        while not done:\n",
    "            a = self.action(s)\n",
    "            s_prime, r, terminated, truncated, info = self.env.step(a)\n",
    "            done = terminated or truncated\n",
    "            self.put(s, a, r, s_prime, done)\n",
    "            s = s_prime\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise environment\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "rld.seed_everything(42, env)\n",
    "\n",
    "# set up recording\n",
    "env = rld.Recorder(env,\n",
    "                   smoothing=10,\n",
    "                   video=True)\n",
    "recorder = env\n",
    "recorder.video = False\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "# clip negative rewards (known trick in Walker environment)\n",
    "env = rld.transparent_wrapper(gym.wrappers.ClipReward)(env, min_reward=-10)\n",
    "\n",
    "# initialise agent\n",
    "agent = Agent(env)\n",
    "agent.target_entropy = 0  # slightly higher entropy to encourage exploration\n",
    "\n",
    "for n_epi in range(2000):\n",
    "\n",
    "    recorder.video = (n_epi + 1) % 100 == 0   # record video every 100 episodes\n",
    "    info = agent.episode()                    # collect episode\n",
    "    env.add_stats(info, ignore_existing=True) # explicitly add stats in case of early termination\n",
    "    tracker.track(info)                       # track statistics\n",
    "    agent.train()                             # train agent\n",
    "\n",
    "    # update progress every 10 episodes\n",
    "    if (n_epi + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
