{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable CUDA (better on Colab/NCC: choose an environment without GPU)\n",
    "import torch\n",
    "import rldurham as rld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "**Reinforcement learning agent**\n",
    "\n",
    "Replace this with your own agent. Also see the SAC implementationi below for a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Agent, self).__init__()\n",
    "        self.discrete_act, self.discrete_obs, self.act_dim, self.obs_dim = rld.env_info(env)\n",
    "\n",
    "    def sample_action(self, s):\n",
    "        return torch.rand(self.act_dim) * 2 - 1 # unifrom random in [-1, 1]\n",
    "\n",
    "    def put_data(self, action, observation, reward):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameliemajor/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/ameliemajor/Desktop/Year 3/Reinforcement Learning/Reinforcement_Code/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFeCAYAAAAYIxzjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIJ5JREFUeJzt3XtsZOd93vHnzH1ILsnl7nK1u9ZqJSfyynJkK46iKjEUK0otx4aNOnXaxLe6qdMUSJoCvSJ1EyTwH0HS2AhgB0XaBjXiIGgN17GQ2nBduYZtxXZQuXHkyFYtWdrVXrkXksslOZzbOcXvnDnDs7NDcmbF4cyZ9/sRjubC4ezL4XDe572c9/WCIAgEAACclRl2AQAAwHARBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcFyu1wc+++xgCwIAQD8ymc1jYmLzyPVcsyHGSwYASIVsNqro7cjnpXJZKhajw/OGXbp0IwwAAEaStfgLhRsPCwF2acEAu4cwAAAYGdbKL5WiVn9c6ccHrf/BIQwAAIbCKner5K3ij8f77bb1CMQVPwFgbxAGAAB7OtnPWv8WACYno9Z/EpX/cBAGAAC7zir95GS/uPvfLhnvHz2EAQDAy2Yt+rjSt8Oux4cFAlr8o40wAAC4JVbJx5P97NJux2P+diA9CAMAgB1Zy96OeLLf1NRmi5/JfulHGAAAdJ3lb637eHGfOAR0Pg7jgTAAAI6zSj2e6Bcv6pMc+8f4IwwAgIOs8o8rfDviMGCXjPe7hzAAAA6Iz++3rn7r8reK3+6LV/ajy99thAEAGDNx5W7d/Vb52+I+FgDiryUvAUMYAICUi9fuj0/1s4OtfNEP3ioAkDKdk/3ig618casIAwAw4qyCj5fytct4ol+8yA/wchEGAGAExeP9+/ZFlb8FgnhlP1r/2G2EAQAYorhyj8f74wCQrPCp/DFohAEAGNJkP2v9xxP+7ACGhTAAAANu+Vul3znZzw7G+zEqCAMAMICtfK2lb9398cI+8WQ/uvwxiggDAPAyWSUf7+Rnl/GiP0z2Q1oQBgCgR3EFb5V/crZ/cnEfKn+kEWEAABKSLfq44rfLeGMf6/63pX2p9DFOCAMAnBBX6lsd8Xh+5302/h9/LzCuCAMAUivZco9P2et2xGP4nWP53Q7ARYQBAEPXucBOZ6s8eW5+fLnVOH23Xfmo5IHtEQYAvGzJCrjb9biLPVmhd1bwyaOffxPAy0cYANBVZ7d6sms9OcHOLuNKPDnW3jn5jsobGF2EAcAxnRPktrudrPQ7AwCb5gDjgzAAjIHkOLu1wre7vtXEua26+AGMP8IAMAK6VbxxhZzctz45zp6cTJc87a3zuZhIB2AnhAFgl+3U2k6OsW93JGfL9/JvAsCtIgwAu8Aq45mZm8fdt5pUBwCjhDAA7BI7J/7AgWGXAgD6xwKbwC4IAunqVWlxMboOAGlCGAB2ie9Lly5JKysEAgDpQhgAdtmFCwQCAOnCnAFgQIGg0Yi2vJ2aGnZpAGB79AwAA3L5snTxonT9+rBLAgDbIwwAA2S9AxYI1tcZNgAwuggDwIA1m9JLL0kbGwQCAKOJMADskdOnpbU1qVoddkkA4EaEAWAPnT0bTS6sVIZdEgDYRBgA9pgNF1ggoIcAwKggDABDUKtF8whsgiEADBthABjixMIXXpDq9WGXBIDrCAPAkJcwth4CW7HQegsAYBgIA8CQWc/A+fPSwgK9BACGgzAAjAg77fDcuai3AAD2EmEAGLEzDU6dIhAA2FuEAWBEzzSwYGCTDAFg0AgDwAj3ENhmRwQCAINGGABG2PJyNLGQPQ0ADBJhABhxdtqhTSwEgEEhDAApsLoa7WtgQwb0EgDYbYQBIEWB4Pnno6EDzjYAsJsIA0CKWK+AzSFYWqKHAMDuIQwAKWRnGdgBALshtyvPAmDPWe+AOXRI8rxhlwZAmhEGgJSyYYLFRSmTkWZnpWyWUADg1jBMAKTclSvRxEKbYMg8AgC3gjAAjAlbi8DWJACAfhEGgDESn2kAAP0gDABjxNYfsLMMrIfAhgwYNgDQCyYQAmMYCM6fjyYUHj8uFYvDLhGAUUfPADCmbOli2/lwfX3YJQEw6ggDwBizYQLrJbAzDQBgK4QBYMw1GtHEwrW1YZcEwKgiDAAOqNejHoLTp9nkCMDNCAOAQ3MIKhXp+9+PwgEAxAgDgIOh4OxZqVoddkkAjArCAOAgCwIXLhAIAEQIA4CjNjaieQQ2wRCA2wgDgMOsZ+DFF5lYCLiOMAA4Lp5YaPMI6CUA3EQYABCylQptPQLONADcQxgA0Hb9ehQIrLcAgDsIAwBuYEsXnzkThQIAbmDXQgBdzzSwwyYVHj4sZWg2AGONP3EAW7p2Tbp8mTMNgHFHGACwraUl6coVAgEwzggDAHa0uBjNIbAtkQGMH+YMAOh5yMDmEUxPS3NzkucNu0QAdgs9AwD6WrHQ5hAsL9NLAIwTwgCAvtmQgfUUEAiA8UAYAHBLLl6MJhcCSD/mDAC4ZTZkYPsa7NsXzSUAMFzWW2fDebVadGnLix89uvP3EQYAvKwPHlvC2PY1sIWJJieZWAgMSrdhOTvl1/7+rOK3Cb4WAuw+O+zxdhAGAOwJ28vAdj08flwqlwkEwG6IK/PkYZW99cbFlf9ubSxGGACwa156Sbr99qiHAEB/rLK3YJ084u5+u4xb/YNAGACwq86di9YhsEBgvQQAurPKvtGIWvd2JK/Ht/cKYQDArrKWiy1fvLIiHTlCIABiVsFbKz9u6dvtuAcgHucfFsIAgIGwDzubR3DihJTLMY8A7k3uq7TG9uPLuNKPx/9HCWEAwMDYh98LL0h33ikVCsMuDbA7glZFnqzYO2f12+WoVfjbIQwAGCj7QLSJhceOMWSAdApalX1yYp918Scn9+3l+P4gEAYADJx9UF64IE1NSbOz9BJg9Cv/emIiX3Jyn13aMW5behMGAOwJaz3ZVshra9F6BNnssEsERHx/c2JfsqUf9waMW8XfDWEAwJ6yD9tTp6J5BLZqITBo8dh9fGkVvY3r28S+eNW+ILGwj4sIAwD2nHW3WiCwBYry+WGXZjwquGRF1nmfhS5XXufkhL54gp9V9lbpx4e19nEjwgCAobAP6PPnozkENpfA9WGDzmVnkzPVt7q90xF3cVsQuO02qVTS2Il/Tmvtx5fxOfzxqn2utvb7QRgAMDTWTRvvemgLFI3TsEG3CnunSjxZ6XeettYtEPTKKknbcnocAkG8RG+3CX5xGED/CAMAhs52PrQPcptYOOrrxidb3fHtzuudu8Z1duF3uz1o8aY2aQoD8az++Lz9eEvezgCFl48wAGAk2IItZ85E6xHcSg9Bt0php/s6N4VJHlbRxK3Nzhnlvf5bo8aGZe64QyoWR2NFyM45DvGs/nhinx2jumLfuCEMABgZdtqhrUdw4EC0FoFVWNu1pLuNs2/VWu9s1btwulgne41s4uarXjWcf7tzmMR+D8kV++zAcBAGAIzckIGFAtv50MJALxPl6C7uj20iNTOzd9vxxgv1xOP78cQ+ZvWPDsIAgJHd+RCDsbAQvcb79+/ec9rzxRV9fBmHgDgQENhGF2EAABxjFbf1vthpnb3OHeisyOPx/WQXf3JXPheHYdKMMAAADlpdjXpfDh7sHgg652NYyz45sc9a/0zsGx+EAQBw1NWr0ZkbNlzQOQEz3pEv2erH+CIMAIDDLl/eDAHJCX5wC2EAABxnPQRw2xgt/gkAAG4FYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADHEQYAAHAcYQAAAMcRBgAAcBxhAAAAxxEGAABwHGEAAADH5YZdAESa9apWFs5q8dyLOn/htJ65eLrv5ygXSnrk9W/U0Xt+WLlCaSDlBACMH8LAgARB0PX+RrWiK6ee1cIL39HjX/+CKvVq9HjfV7O2ofpGRcVqRQ/UNnRQCo9sL/+epKe9jD7yzS+rODmtB++8R2/42z+rQ698Tfsxnuft2s8HABgfXrBVrdXh2WcHX5hxYS/p0uIlXfzuN3Xh+3+jJ7/1pBaWroRjMhmrtoMgrPwf9Js6JumQpDlJ+db3e60AEFfdvVTh9kv0rYdB0pKkT3qeLmVzKpcm9MYfeaN+6JF3aOrgEU2WJ5Uvlgf40wMARsnJkzs/hjAwAPaS/uIv/LjuqEWtfvs93Nmq8Gf2sDvGfrHPSfp8KySsSnr40Xfqzgd+UvvKkzp67E4VypN7VBoAwKiGAYYJBsSq2F8dchmsR+Hu1lGTdFHSt774KT35xU9pfW5exx74SU3uP6TXnrhHR+++Tzl6DADASYQBRxQkHZd0eysYXFi8pMX/+V91XtJ/PvwKzR69Uyfmj+nBH3lER+5+rTK5eNACADDuCAOOsd6CoqQTrcOmFz64cFaVhbP6cjan3/n6F1QoT+hNr35A9z/yDs2dOBlOPIwPAMD4IQw4rtCawGjzC97dbChYWdSllUX94cI5ffbLj6vheXrkoTfrx9/+fhWKE5reN8MERAAYM4QBhKzNH7f7j0j6TQWq+4EuSPrak5/Vf3nys6qVp/TAoz+jBx59p+bn7TwIAMA4IAxgS/nWPAM76pJOV1b1v//HHys3d1iPPfZzwy4eAGCXsBwxemJrGCxLujzsggAAdh09A9hyASNbJeGUpC94GV3NZjU5NaN3vfOf6ORDjw27iACAXUQYQMgq/+uS1iVdkfTXmaxemJtXtlDSTz/0Jr3hre9VNl/krAIAGEOEAcetSPpeqxfge6VJrb/iTu07cET7pqb1Gz/9Hs0csRkDAIBxRhhwcAhgTdJ3JJ2RtCip9KrX6fbXPKiHDtymH3zlvZo9ckKZbC/bIwEAxgFhwBEVSS9K+mLr+v4jd4Rd/9OHb9fRuXlNHzzCqoMA4CjCwIDUimX9u9buhK9pNnW3As23NitKvui5xDn+3i62/uuteQCnJX3Dy+i5fF5NL6N/8I5f1D0/8XblslmVShPKZHkLAIDr2LVwQPxGQwvPPa1LL35XX/2bv9SFa1fD+y0cNKrrqlXW1aht6OGNdR2UNJvYwjjX2t0w3zp63cJ4tbV9sZ0C+ESxrMr+g8qXJnXPXa/WO/7OBzQxZ3FETAAEAIecZAvj0dOoVbWy8JKWzp/W4pXz+tZ5O3lP8psNVa4taW35shrXrurQtcUwEMz0sRjEqWxO10+c1OyRO3TvK+7Sa17/E5o5coLKHwAcdpIwkK6ehMq1q1pduqyl5Ss6u2wn+EU2Ntb1iU98WAcPHlQud2O3/srKiiYnZ/XmN/+8pqdmdO9dr9b+oycY/wcA9BwGGDAeEZlcTpMHDofHYfvlJb527dqiPv7x31W5XNbExMQN37e+vh4GhDe84S2anbUBBwAA+sNyxClhHTiNRuOm+6empnTu3CnVarWhlAsAkH6EgRTIZDKanp4OhwQ6bQ4b9DTaAwDATQgDKWATADvnCiSDgh0AANwqapGU2CoMZLPZMCzM//t/Jfm2sgAAACMeBmzsO3lgZ3Zm4E49A1NPfMZe3D0vGwAg/fY8DPi+r7/1ozN67Ecn9Z8++kFdvnyhfays2HI56OR5Gc3MzLTD1I1fi9YQWJg7qOzS5umIAAD0aiinFh7wK/qLexr65Od+Wx/8k99u33/X6x/Rw+/9tfbtqalp3Xffg3Kd1fc2HNBsNsMwZdc7w8BTv/YRvf99b9SLn/vuEEsKAEijoa4z8PcORkfsmYUv6fEPfql925s7pm889svt27aW/vve98/kHi8MABYEthpaubRwRgFnFAAAbsFILTp070R0xJYb5/QXj//b9u1qtqBffeor7ds2Vv57v/enyjmw2l6yZ6CbS0uXdfUD/0b7P/4RLb3/n+95+QAA6TVSYaDTbE566/7N282gpvvOf7p929rBP/P2p9WQp3e965f1nvf8U42jYnFCjz76Xn372/+6axiYnZ3V177+hCofeq/mP/qb4WZFAACMRRio+dJF24u35XqQ0QeuHW/fzngZ/ffH/69yucJYn2tvP9vc3HzYO1CpVFQqlW74ej6fD/cvYJQAAJD6MHChJj29vnn7amFWnzvycPv25OS0Pv+7n5CLbKKghYFucwbstEP7uj8xpcb8EeVfel714z8wlHICANJnqGHgqyvS164n7jh+n4JHfrZ989ChI/qDd/6joZRtFHsHkmcRJMVrEDQOH1XldQ9p3xOf0eIv/Ms9LiEAIK2GEgYu16V3Pyf90GPv0b0/9a72/fPzR3Ty5OuGUaTU9AxstwohAACpCQOzh2/Xb3ziK+E6AnZgZ3YmQbVaDecL2FBBsvKPQ8LGxoayj7xNBz/2Wyr/1ddUuf/HhlhiAEBaDGXWXTab0223vYIg0CcLARYKOkXBINDFi6cUlCYkvymvtjGUMgIA0md8p+CP4ZwBO2ug0Wh0nURody0snA2vV++5X8XnnpFXJRAAAHZGGEgJmyQ4MTER9gx0DwOBLl8+H15fedu7NfW//kyZ1ZUhlBQAkDYjdWohtmZDAdY7sLS0pLW1tZsmDNbrdV29enFo5QMApBc9AylhkwSPHj2qcrmsY8eO6a677gqPEydOhF8/ePCAnnrqq+3Hn/vYp3X7Bx5jW2MAwI7oGUgJq9PjIQIbMmivLdBohJeFQkGNxuZyjf6+WWWusyU0AGBn9AykRBD44VBA53yBeCdDm1zY6fqb/q6mnvjMHpYSAJBGhIGUsAq/VqvdtFGR3R/3FnS68iu/pYP/4UN7WEoAQBoRBlLCgsDCwkLYA5BciTDuGThz5szW38y8AQDANggDKesZsDCQ3KExDgMf/vCf6fd//8YhgaA8oYVf/5gOf+hXhlBiAEBaEAZSpnN/AptAWCgUNTExpXJ58sYH206GxZK8amVvCwkASBXCQMrE2xXH1tfXdeTI8fBsAgAAbgVhIGW67VA4M3NAmUz3HQ3rx06oduJuTXzji3tUQgBA2hAGUqBaXddXvvLfwh0Lk/MFYtPT+7fc3tifmVNzbl6FU8/tQUkBAGlEGEgBmyS4uLjQXmOgs2dgny0wtEXPQPj9pbI8W5CoXht4WQEA6UMYSAELAbbgULfti3sJA9ff+vMqPP+MSt/9qwGWEgCQVoSBlLDTCrcOA3PbhgEAALZDGEjJMIHtVrhVGJiZmdtyzkBs+e//kmY/9UfyKusDKiUAIK0IA6kQaHV1NRwusF0L2/cGQRgUcrn8TfMIOlXvuV/FZ/86mjsAAEACYSAlrOK3MwmSPQDxvgS9aswfVW7h3IBKCABIK8JAilgYSJ5aGPcM9OrcRz+t23/pLQMqHQAgrQgDDoUBAAC6IQyMOKvwv/SlP9XExMRNqw9aEOg3DFz89Y9p/nf+xQBKCgBIK8JACpw58712CEiGgb57BjxPlft/TKWn/3IQxQQApBRhICVrDNjuhJ0sCNj6AjudVggAwHYIAylgqw92CwMWEvbvn9f+/Yd6fq4gX1D15OtUfOabu1xKAEBaEQZSYHl5uWsYMMViOVxnoFfBxJRW3vJzmnn8j3exhACANCMMpMDq6vVw0aGpqamuYSCfLwylXACA8UAYSAFbV8jmBxQKN1f65fJk32Gg8toHVT96h/Z9/pO7WEpg+OKFuILAl98+mmqGR0ON8KirrqoaqreOhprtoyk/PKL/gvCI/gNG6T2++d6O3tPh+zqoqRZUVQs2VA0qWslc1dnC8z09b27gJceu6Fx9MBkGCoVif09mwwqZrLxadfcKiFSwyi38z2vefOk1VVNVmUZGWT8nL/wvs3npeTfd53sNZb3ehqnCijUI5AXWBomr2LiibV0LNq9XMxvKZwsKvGCzUvY2K2e/fd0P7/cbvoKar3qrsq+3PhTjy1pQUdWvhNfPFV/Q3Oy8St6U8ioor6LyQeIyKIbXc637Ms2MihtleYHaP/vmf9nwyHq58PC8jHJeXjkVWo/dfqlwuCcIgjB8bnjr8jKevak2w6e9l1vv9fi+6L3uq+HXFVSDsKLfCFZV8Ve1ruta869rPTxWtNZc1qpvx5L8ww0paOqn9Oc7lokwkBL2QZxccCg2MTGlfL7PMGB7FfzgvSp9+yllri3Kn5nbpVJiT1sHCtRQrdXSrYWtXGsZRK1du78W3r9WWFF2MquG11DDq6tpXw9bEvb1DdV8OyqqBuu6GlxUfimn0saUspmocrvxyLevZ7yc1ifXtW9yuqcy172a/Ou+cus5BdaiCcNBM2zdhK3xVkvHrlvL/tLUec0W5+QHjbD1E7bcw8t6dBnU2/fZz1Jb3VDmakaZIKOccsoEWWUCT57vhZW4mr4CO/yG6sWGruX+n5SVgtZh/aTt64n7w9s2ZWdB7crfXoecV1A+U1QhU1YxW1YpO6VSdp+8iZz2zc5pKj8bBoucX1DBL6mkifCyEJRU8IvhkQ9K4WMsD3UNRzf0TNjvuyE/44dhoxc1bSjXyLUiYDPR8xH1lrSvh69kXdXiukqFybA83WzGGk8Vram8NqViUGqHn5zsMt++ndWNa6O4wg/8MJBapV2V/W1taCNYCytwu25/lxv+qhbyZ5QrZcOK397D9p5uxH/Dydt2qZpWa8vKnssoG2RVsMAa5JWz4O57ytg+dhaIGw2V6jXlag0F32lGv7NXavfCwKeqf7D9A26T1ENvdeD7Wg2Wd36+JKurbh4u39pLfTx2n6T9fTzelvbvvnngzUqS5vt47gWrpTvvDJR5s6faf6zp/PnzN/xh2dkE5Yef1Z8f/6OdtzBetMkHids/LE3W/482qnU1q13CwPE+yr3aev5eHYs+bHtSbb0uvTpsEyl6fKz9HvvZqmF/6/0yiPehvb/3Rx+24W/YKjH7L+honSsTVvjegtRo1KNWpwWDcAGqhhrNuprNqurNDdUa66rWV3WtfFWFfFbNZt0+pZT1s8r4GXlN+wAJmyStD5GoMq5uBFpthkVoFyisG+yyXcDoul+SMhO9/Yjh91Ykr7VxZlhBh1+If+bN28YvShXvhehxfutxdum3vrd1mfejoxx+3ZcX/UDblqX3Kbc3FElBJmq5KVNX4K2rnpFqnrRqQcJen4zk5yRvWgqKFiQy4QTfXL6kQmFSE6VZlQrTypdLyuUKUjajwIp7LkwDm/9eoockvN4a8mhka2rO+sqXevvo3shuqHAqH70/rPL3Gwr8KICFQcyut243MjVVZiqayLd+ofZ6dqvIw7s8refWNX11TmVNKZctKZstKJuNQqJnDRdrwCijvFdUwSuqXqppemYuCmn2XxjWEtcTlxv+ukrrE5pqzqpgvTX2HCqFz5NX6WWFjLhVbj1GVQvDYS/SRhicwi72jstr2SsqHZro6d+zYFVZX5O/ZK93I/ybq9XXtVG7pkp1WdX6mur1Dfl+XRnfk7KBLNe13892NBPv9ebmez7jS9OBvfWsgrcH1cJ/c7t3ej+vkBf0uNPNP/yTt+1c8fXyAR8EWnn6uqZ/oLfWRKjYZx/GWh+PtU+Ffobc7YOs1+HDbOt16dVG96ARNANtnLIv3ix/OKfc4R4+2ux908+GhZN9PLbefl/2ZqKPd6m9Ht1/9Jf3PlTr99jPjs6FPmuRft6HuajysNfFS1S+9gFklUxY6YcVsadG1ld2SfIaQbujOgoN0TfaB0tcgVglYN3s8QdK62mxB+LfYdjj0Dq8XEZeNiNlPAWZIGzlW9u8HZAS/7/xiRJX8r2/x8P3Tuszq/2sW4Uw08+SJfbzNKOgGr5p47AYdnG3+jTszWjBIJdV7YCvUrYQ/uz2+LB7/IajFT4ynmrFpnJLBRVrE8rk8spkswrsebKBml70vrdwUFJZJW9Sq+Vrmj10MOoZ86xlbZcNNb1GOJRl3ezhkQnC112XfAV22LbwjdZlsxn1HMXXbUG3ZlPVYlWFfG8VkP28duZX7qrCXik7wr87+zv0bYisVfEnfx974A//8S4OExR2+tDs+UPV08Gj0/19CPfz2FFzfTeexFPxyObWxTe5rN03yNec577J1vmyo7nckUeiVrC/5XMlGvLYY+2K1lp3ccivbv6+4t9N9qbf0Q6tjYoGZ/sOlS5ufO8lf45s+yeJfub88haF7/IGjTv3GkH3bGR1bCUnLWctVHmqHgp06UUbEgqiBkTYwraZ1ze2rtst71ZPU/yEXqIonQFpwu4Iem/txH+fXsomnTJnABgBVNhu8lz6+baqG3eqM4MtnjMMWFEzuxAOgQ6o8g3kBE4tBADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAADAcYQBAAAc5wVBEAy7EAAAYHjoGQAAwHGEAQAAHEcYAADAcYQBAAAcRxgAAMBxhAEAABxHGAAAwHGEAQAAHEcYAABAbvv/PihFBqQvvr4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"lspf78-agent-video\",  # prefix for videos (replace lspf78 with your username)\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image (reset just to render image)\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect episodes and train the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rDl6ViIDlVOk",
    "outputId": "731e4ce7-c98d-4bde-8a2c-fbdf1410e24f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/Users/ameliemajor/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'torch.Tensor'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m action = agent.sample_action(observation)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# take action in the environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m observation, reward, terminated, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# remember\u001b[39;00m\n\u001b[32m     35\u001b[39m agent.put_data(action, observation, reward)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/rldurham/__init__.py:371\u001b[39m, in \u001b[36mRecorder.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m    370\u001b[39m     \u001b[38;5;28mself\u001b[39m._episode_started = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m     reward = \u001b[38;5;28mfloat\u001b[39m(reward)  \u001b[38;5;66;03m# convert to float in case it is tensor/array\u001b[39;00m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m._episode_reward_sum += reward\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:363\u001b[39m, in \u001b[36mRecordVideo.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    361\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    362\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     obs, rew, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28mself\u001b[39m.step_id += \u001b[32m1\u001b[39m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger(\u001b[38;5;28mself\u001b[39m.step_id):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/rldurham/__init__.py:199\u001b[39m, in \u001b[36mRLDurhamEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m._unscaled_reward = \u001b[38;5;28mfloat\u001b[39m(reward)  \u001b[38;5;66;03m# convert to float in case it is tensor/array\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/rldurham/bipedal_walker.py:604\u001b[39m, in \u001b[36mBipedalWalker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    601\u001b[39m     reward = shaping - \u001b[38;5;28mself\u001b[39m.prev_shaping\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.prev_shaping = shaping\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.00035\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mMOTORS_TORQUE\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    606\u001b[39m     \u001b[38;5;66;03m# normalized to about -50.0 using heuristic, more optimal agent should spend less\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/torch/_tensor.py:1214\u001b[39m, in \u001b[36mTensor.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim() == \u001b[32m0\u001b[39m:\n\u001b[32m   1213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33miteration over a 0-d tensor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1215\u001b[39m     warnings.warn(\n\u001b[32m   1216\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1217\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing a tensor of different shape won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt change the number of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1221\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1222\u001b[39m     )\n\u001b[32m   1223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m.unbind(\u001b[32m0\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "# initialise agent\n",
    "agent = Agent(env)\n",
    "max_episodes = 10\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "# switch video recording off (only switch on every x episodes as this is slow)\n",
    "env.video = False\n",
    "\n",
    "# training procedure\n",
    "for episode in range(max_episodes):\n",
    "    \n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    env.info = True                 # usually tracking every episode is fine\n",
    "    env.video = episode % 100 == 0  # record videos every 100 episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # select the agent action\n",
    "        action = agent.sample_action(observation)\n",
    "\n",
    "        # take action in the environment\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # remember\n",
    "        agent.put_data(action, observation, reward)\n",
    "\n",
    "        # check whether done\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # train the agent after each episode\n",
    "    agent.train()\n",
    "            \n",
    "    # track and plot statistics\n",
    "    tracker.track(info)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file (for coursework)\n",
    "env.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")  # replace xxxx00 with your username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (designed for the orignal BipedalWalker environment, but should also work fine for rldurham/Walker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for _ in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3 Implementation\n",
    "\n",
    "This section replaces the SAC baseline with **TD3 (Twin Delayed DDPG)**, a strong off-policy algorithm for **continuous** control.\n",
    "\n",
    "Key TD3 ideas (compared to DDPG):\n",
    "\n",
    "- **Twin critics** \\(Q_1, Q_2\\) and use \\(\\min(Q_1, Q_2)\\) for targets to reduce overestimation.\n",
    "- **Target policy smoothing**: add clipped noise to the *target* action when computing the bootstrap target.\n",
    "- **Delayed policy updates**: update the actor (policy) less frequently than the critics for stability.\n",
    "- **Soft target updates** (Polyak averaging) for both actor and critics.\n",
    "\n",
    "The implementation below is intentionally compact and easy to tune for the coursework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Replay Buffer\n",
    "# -----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim: int, act_dim: int, size: int = 1_000_000):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "        self.done_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "        self.max_size = size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # OU noise parameters\n",
    "def reset_episode(self):\n",
    "    \"\"\"Reset per-episode state (e.g., OU noise). Call at episode start.\"\"\"\n",
    "    self.ou_x[:] = 0.0\n",
    "\n",
    "def _ou_noise(self, sigma: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ornstein–Uhlenbeck noise with per-dimension sigma (in action units).\"\"\"\n",
    "    dx = self.ou_theta * (-self.ou_x) + sigma * np.random.randn(self.act_dim).astype(np.float32)\n",
    "    self.ou_x = self.ou_x + dx\n",
    "    return self.ou_x\n",
    "def store(self, obs, act, rew, obs2, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.obs2_buf[self.ptr] = obs2\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "def sample_batch(self, batch_size: int, device: torch.device):\n",
    "    idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "    return dict(\n",
    "        obs=torch.as_tensor(self.obs_buf[idxs], device=device),\n",
    "        act=torch.as_tensor(self.act_buf[idxs], device=device),\n",
    "        rew=torch.as_tensor(self.rew_buf[idxs], device=device),\n",
    "        obs2=torch.as_tensor(self.obs2_buf[idxs], device=device),\n",
    "        done=torch.as_tensor(self.done_buf[idxs], device=device),\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Networks\n",
    "# -----------------------------\n",
    "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, act_limit: np.ndarray):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim, 256, 256, act_dim], activation=nn.ReLU, output_activation=nn.Tanh)\n",
    "        self.register_buffer(\"act_limit\", torch.as_tensor(act_limit, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(obs) * self.act_limit\n",
    "\n",
    "\n",
    "class CriticTwin(nn.Module):\n",
    "    # Two Q networks in one module\n",
    "    def __init__(self, obs_dim: int, act_dim: int):\n",
    "        super().__init__()\n",
    "        self.q1 = mlp([obs_dim + act_dim, 256, 256, 1], activation=nn.ReLU)\n",
    "        self.q2 = mlp([obs_dim + act_dim, 256, 256, 1], activation=nn.ReLU)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: torch.Tensor):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q1(x), self.q2(x)\n",
    "\n",
    "    def q1_only(self, obs: torch.Tensor, act: torch.Tensor):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q1(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# TD3 Agent\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class TD3Config:\n",
    "    gamma: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    actor_lr: float = 1e-3\n",
    "    critic_lr: float = 1e-3\n",
    "\n",
    "    buffer_size: int = 500_000\n",
    "    batch_size: int = 256\n",
    "\n",
    "    # exploration (interaction) noise\n",
    "    exploration_noise: float = 0.1\n",
    "\n",
    "    # target policy smoothing noise\n",
    "    policy_noise: float = 0.2\n",
    "    noise_clip: float = 0.5\n",
    "\n",
    "    # delayed policy update\n",
    "    policy_delay: int = 2\n",
    "\n",
    "    # warmup steps of random actions\n",
    "    start_steps: int = 5_000\n",
    "\n",
    "    # how many gradient steps per environment step (can tune)\n",
    "    update_every: int = 1\n",
    "    gradient_steps: int = 1\n",
    "\n",
    "\n",
    "class TD3Agent(nn.Module):\n",
    "    def __init__(self, env, cfg: TD3Config = TD3Config(), device: str = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        # environment dims\n",
    "        self.discrete_act, self.discrete_obs, self.act_dim, self.obs_dim = rld.env_info(env)\n",
    "        assert not self.discrete_act and not self.discrete_obs, \"This TD3 assumes continuous actions and observations.\"\n",
    "\n",
    "        # action scaling (handles non-[-1,1] ranges)\n",
    "        self.act_limit = env.action_space.high.astype(np.float32)\n",
    "        # Temporally correlated exploration noise (Ornstein–Uhlenbeck)\n",
    "        # Helps produce smoother actions/gaits than i.i.d. Gaussian noise.\n",
    "        self.ou_x = np.zeros(self.act_dim, dtype=np.float32)\n",
    "        self.ou_theta = 0.15\n",
    "        self.action_cap = 0.7  # fraction of act_limit used during both training & evaluation\n",
    "\n",
    "                # networks\n",
    "        self.actor = Actor(self.obs_dim, self.act_dim, self.act_limit).to(self.device)\n",
    "        self.actor_targ = Actor(self.obs_dim, self.act_dim, self.act_limit).to(self.device)\n",
    "        self.critic = CriticTwin(self.obs_dim, self.act_dim).to(self.device)\n",
    "        self.critic_targ = CriticTwin(self.obs_dim, self.act_dim).to(self.device)\n",
    "\n",
    "        self.actor_targ.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_targ.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=self.cfg.actor_lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=self.cfg.critic_lr)\n",
    "\n",
    "        self.replay = ReplayBuffer(self.obs_dim, self.act_dim, size=self.cfg.buffer_size)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.update_steps = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, obs: np.ndarray, explore: bool = True) -> np.ndarray:\n",
    "        self.total_steps += 1\n",
    "\n",
    "        # Limit action magnitude to reduce bang-bang / hopping behaviours.\n",
    "        max_act = self.action_cap * self.act_limit\n",
    "\n",
    "        # warmup: random actions (within capped range)\n",
    "        if explore and self.total_steps < self.cfg.start_steps:\n",
    "            return np.random.uniform(-max_act, max_act).astype(np.float32)\n",
    "\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        act = self.actor(obs_t).cpu().numpy()[0]\n",
    "\n",
    "        # Always apply action cap (during both training and evaluation for consistency)\n",
    "        act = np.clip(act, -max_act, max_act)\n",
    "\n",
    "        if explore:\n",
    "            # OU exploration noise (smoother than i.i.d. Gaussian)\n",
    "            sigma = (self.cfg.exploration_noise * max_act).astype(np.float32)\n",
    "            act = act + self._ou_noise(sigma=sigma)\n",
    "            act = np.clip(act, -max_act, max_act)\n",
    "\n",
    "        return act.astype(np.float32)\n",
    "\n",
    "\n",
    "    def store(self, obs, act, rew, obs2, done):\n",
    "        self.replay.store(obs, act, np.float32(rew), obs2, np.float32(done))\n",
    "\n",
    "    def _soft_update(self, net, net_targ):\n",
    "        tau = self.cfg.tau\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(net.parameters(), net_targ.parameters()):\n",
    "                p_targ.data.mul_(1 - tau)\n",
    "                p_targ.data.add_(tau * p.data)\n",
    "\n",
    "    def train_step(self):\n",
    "        if self.replay.size < max(self.cfg.batch_size, 1_000):\n",
    "            return\n",
    "\n",
    "        for _ in range(self.cfg.gradient_steps):\n",
    "            batch = self.replay.sample_batch(self.cfg.batch_size, self.device)\n",
    "            obs, act, rew, obs2, done = batch[\"obs\"], batch[\"act\"], batch[\"rew\"], batch[\"obs2\"], batch[\"done\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # target policy smoothing\n",
    "                noise = torch.randn_like(act) * self.cfg.policy_noise\n",
    "                noise = torch.clamp(noise, -self.cfg.noise_clip, self.cfg.noise_clip)\n",
    "                noise = noise * self.actor.act_limit  # scale by action limits\n",
    "\n",
    "                act2 = self.actor_targ(obs2)\n",
    "                act2 = torch.clamp(act2 + noise, -self.actor.act_limit, self.actor.act_limit)\n",
    "\n",
    "                q1_t, q2_t = self.critic_targ(obs2, act2)\n",
    "                q_t = torch.min(q1_t, q2_t)\n",
    "                backup = rew + self.cfg.gamma * (1.0 - done) * q_t\n",
    "\n",
    "            # critic update\n",
    "            q1, q2 = self.critic(obs, act)\n",
    "            critic_loss = F.mse_loss(q1, backup) + F.mse_loss(q2, backup)\n",
    "\n",
    "            self.critic_opt.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_opt.step()\n",
    "\n",
    "            # delayed actor update\n",
    "            if self.update_steps % self.cfg.policy_delay == 0:\n",
    "                actor_loss = -self.critic.q1_only(obs, self.actor(obs)).mean()\n",
    "\n",
    "                self.actor_opt.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_opt.step()\n",
    "\n",
    "                self._soft_update(self.actor, self.actor_targ)\n",
    "                self._soft_update(self.critic, self.critic_targ)\n",
    "\n",
    "            self.update_steps += 1\n",
    "\n",
    "    def episode(self, env, max_steps: int = 1000):\n",
    "        obs, info = env.reset()\n",
    "        self.reset_episode()\n",
    "        done = False\n",
    "        ep_ret = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        while not done and ep_len < max_steps:\n",
    "            act = self.select_action(obs, explore=True)\n",
    "            obs2, rew, terminated, truncated, info = env.step(act)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            self.store(obs, act, rew, obs2, float(done))\n",
    "            obs = obs2\n",
    "\n",
    "            ep_ret += float(rew)\n",
    "            ep_len += 1\n",
    "\n",
    "            if self.total_steps % self.cfg.update_every == 0:\n",
    "                self.train_step()\n",
    "\n",
    "        info = dict(info) if isinstance(info, dict) else {}\n",
    "        info.setdefault(\"r_sum\", ep_ret)\n",
    "        info.setdefault(\"t\", ep_len)\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/Users/ameliemajor/miniforge3/envs/gymnasium/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:434: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TD3Agent' object has no attribute 'reset_episode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_epi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m):\n\u001b[32m     33\u001b[39m     recorder.video = (n_epi + \u001b[32m1\u001b[39m) % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m   \u001b[38;5;66;03m# record video every 100 episodes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     info = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m                 \u001b[38;5;66;03m# collect one episode (+ train along the way)\u001b[39;00m\n\u001b[32m     35\u001b[39m     env.add_stats(info, ignore_existing=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# explicitly add stats in case of early termination\u001b[39;00m\n\u001b[32m     36\u001b[39m     tracker.track(info)                       \u001b[38;5;66;03m# track statistics\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 236\u001b[39m, in \u001b[36mTD3Agent.episode\u001b[39m\u001b[34m(self, env, max_steps)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mepisode\u001b[39m(\u001b[38;5;28mself\u001b[39m, env, max_steps: \u001b[38;5;28mint\u001b[39m = \u001b[32m1000\u001b[39m):\n\u001b[32m    235\u001b[39m     obs, info = env.reset()\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset_episode\u001b[49m()\n\u001b[32m    237\u001b[39m     done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    238\u001b[39m     ep_ret = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gymnasium/lib/python3.11/site-packages/torch/nn/modules/module.py:1965\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1964\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1966\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1967\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'TD3Agent' object has no attribute 'reset_episode'"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialise environment\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "rld.seed_everything(42, env)\n",
    "\n",
    "# set up recording\n",
    "env = rld.Recorder(env, smoothing=10, video=True)\n",
    "recorder = env\n",
    "recorder.video = False\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "# clip negative rewards (known trick in Walker environment)\n",
    "env = rld.transparent_wrapper(gym.wrappers.ClipReward)(env, min_reward=-10)\n",
    "\n",
    "# initialise TD3 agent (feel free to tune the config)\n",
    "cfg = TD3Config(\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    actor_lr=3e-4,        # ↓ big one\n",
    "    critic_lr=1e-3,\n",
    "    buffer_size=500_000,\n",
    "    batch_size=256,\n",
    "    exploration_noise=0.08,  # slightly ↓\n",
    "    policy_noise=0.15,       # ↓\n",
    "    noise_clip=0.3,          # ↓\n",
    "    policy_delay=3,          # ↑ smoother actor updates\n",
    "    start_steps=5_000,\n",
    "    update_every=1,\n",
    "    gradient_steps=1,\n",
    ")\n",
    "agent = TD3Agent(env, cfg=cfg, device=\"cpu\")\n",
    "for n_epi in range(1000):\n",
    "\n",
    "    recorder.video = (n_epi + 1) % 100 == 0   # record video every 100 episodes\n",
    "    info = agent.episode(env)                 # collect one episode (+ train along the way)\n",
    "    env.add_stats(info, ignore_existing=True) # explicitly add stats in case of early termination\n",
    "    tracker.track(info)                       # track statistics\n",
    "\n",
    "    # update progress every 10 episodes\n",
    "    if (n_epi + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
