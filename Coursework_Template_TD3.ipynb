{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coursework Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Dependencies and imports**\n",
        "\n",
        "This can take a minute..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable CUDA (better on Colab/NCC: choose an environment without GPU)\n",
        "import torch\n",
        "import rldurham as rld"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHtclV_30Re"
      },
      "source": [
        "**Reinforcement learning agent**\n",
        "\n",
        "Replace this with your own agent. Also see the SAC implementationi below for a starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jXNHP8_U-rn"
      },
      "outputs": [],
      "source": [
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(Agent, self).__init__()\n",
        "        self.discrete_act, self.discrete_obs, self.act_dim, self.obs_dim = rld.env_info(env)\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        return torch.rand(self.act_dim) * 2 - 1 # unifrom random in [-1, 1]\n",
        "\n",
        "    def put_data(self, action, observation, reward):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4ZjXmyrHo"
      },
      "source": [
        "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xrcek4hxDXl"
      },
      "outputs": [],
      "source": [
        "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
        "# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
        "\n",
        "# get statistics, logs, and videos\n",
        "env = rld.Recorder(\n",
        "    env,\n",
        "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
        "    video=True,                         # enable recording videos\n",
        "    video_folder=\"videos\",              # folder for videos\n",
        "    video_prefix=\"lspf78-agent-video\",  # prefix for videos (replace lspf78 with your username)\n",
        "    logs=True,                          # keep logs\n",
        ")\n",
        "\n",
        "# training on CPU recommended\n",
        "rld.check_device()\n",
        "\n",
        "# environment info\n",
        "rld.env_info(env, print_out=True)\n",
        "\n",
        "# render start image (reset just to render image)\n",
        "env.reset(seed=42)\n",
        "rld.render(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Collect episodes and train the agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "rDl6ViIDlVOk",
        "outputId": "731e4ce7-c98d-4bde-8a2c-fbdf1410e24f"
      },
      "outputs": [],
      "source": [
        "# in the submission please use seed_everything with seed 42 for verification\n",
        "seed, observation, info = rld.seed_everything(42, env)\n",
        "\n",
        "# initialise agent\n",
        "agent = Agent(env)\n",
        "max_episodes = 10\n",
        "\n",
        "# track statistics for plotting\n",
        "tracker = rld.InfoTracker()\n",
        "\n",
        "# switch video recording off (only switch on every x episodes as this is slow)\n",
        "env.video = False\n",
        "\n",
        "# training procedure\n",
        "for episode in range(max_episodes):\n",
        "    \n",
        "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
        "    env.info = True                 # usually tracking every episode is fine\n",
        "    env.video = episode % 100 == 0  # record videos every 100 episodes (set BEFORE calling reset!)\n",
        "\n",
        "    # reset for new episode\n",
        "    observation, info = env.reset()\n",
        "\n",
        "    # run episode\n",
        "    done = False\n",
        "    while not done:\n",
        "        \n",
        "        # select the agent action\n",
        "        action = agent.sample_action(observation)\n",
        "\n",
        "        # take action in the environment\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # remember\n",
        "        agent.put_data(action, observation, reward)\n",
        "\n",
        "        # check whether done\n",
        "        done = terminated or truncated\n",
        "\n",
        "    # train the agent after each episode\n",
        "    agent.train()\n",
        "            \n",
        "    # track and plot statistics\n",
        "    tracker.track(info)\n",
        "    if (episode + 1) % 10 == 0:\n",
        "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
        "\n",
        "# don't forget to close environment (e.g. triggers last video save)\n",
        "env.close()\n",
        "\n",
        "# write log file (for coursework)\n",
        "env.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")  # replace xxxx00 with your username"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Heuristic demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (designed for the orignal BipedalWalker environment, but should also work fine for rldurham/Walker)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
        "\n",
        "env = rld.make(\n",
        "    \"rldurham/Walker\",\n",
        "    # \"BipedalWalker-v3\",\n",
        "    render_mode=\"human\",\n",
        "    # render_mode=\"rgb_array\",\n",
        "    hardcore=False,\n",
        "    # hardcore=True,\n",
        ")\n",
        "_, obs, info = rld.seed_everything(42, env)\n",
        "\n",
        "heuristics = BipedalWalkerHeuristics()\n",
        "\n",
        "act = heuristics.step_heuristic(obs)\n",
        "for _ in range(500):\n",
        "    obs, rew, terminated, truncated, info = env.step(act)\n",
        "    act = heuristics.step_heuristic(obs)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "    if env.render_mode == \"rgb_array\":\n",
        "        rld.render(env, clear=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TD3 Implementation\n",
        "\n",
        "This section replaces the SAC baseline with **TD3 (Twin Delayed DDPG)**, a strong off-policy algorithm for **continuous** control.\n",
        "\n",
        "Key TD3 ideas (compared to DDPG):\n",
        "\n",
        "- **Twin critics** \\(Q_1, Q_2\\) and use \\(\\min(Q_1, Q_2)\\) for targets to reduce overestimation.\n",
        "- **Target policy smoothing**: add clipped noise to the *target* action when computing the bootstrap target.\n",
        "- **Delayed policy updates**: update the actor (policy) less frequently than the critics for stability.\n",
        "- **Soft target updates** (Polyak averaging) for both actor and critics.\n",
        "\n",
        "The implementation below is intentionally compact and easy to tune for the coursework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Replay Buffer\n",
        "# -----------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, obs_dim: int, act_dim: int, size: int = 1_000_000):\n",
        "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "        self.obs2_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
        "        self.rew_buf = np.zeros((size, 1), dtype=np.float32)\n",
        "        self.done_buf = np.zeros((size, 1), dtype=np.float32)\n",
        "        self.max_size = size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def store(self, obs, act, rew, obs2, done):\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.act_buf[self.ptr] = act\n",
        "        self.rew_buf[self.ptr] = rew\n",
        "        self.obs2_buf[self.ptr] = obs2\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self, batch_size: int, device: torch.device):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        return dict(\n",
        "            obs=torch.as_tensor(self.obs_buf[idxs], device=device),\n",
        "            act=torch.as_tensor(self.act_buf[idxs], device=device),\n",
        "            rew=torch.as_tensor(self.rew_buf[idxs], device=device),\n",
        "            obs2=torch.as_tensor(self.obs2_buf[idxs], device=device),\n",
        "            done=torch.as_tensor(self.done_buf[idxs], device=device),\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Networks\n",
        "# -----------------------------\n",
        "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        act = activation if j < len(sizes) - 2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim: int, act_dim: int, act_limit: np.ndarray):\n",
        "        super().__init__()\n",
        "        self.net = mlp([obs_dim, 256, 256, act_dim], activation=nn.ReLU, output_activation=nn.Tanh)\n",
        "        self.register_buffer(\"act_limit\", torch.as_tensor(act_limit, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(obs) * self.act_limit\n",
        "\n",
        "\n",
        "class CriticTwin(nn.Module):\n",
        "    # Two Q networks in one module\n",
        "    def __init__(self, obs_dim: int, act_dim: int):\n",
        "        super().__init__()\n",
        "        self.q1 = mlp([obs_dim + act_dim, 256, 256, 1], activation=nn.ReLU)\n",
        "        self.q2 = mlp([obs_dim + act_dim, 256, 256, 1], activation=nn.ReLU)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, act: torch.Tensor):\n",
        "        x = torch.cat([obs, act], dim=-1)\n",
        "        return self.q1(x), self.q2(x)\n",
        "\n",
        "    def q1_only(self, obs: torch.Tensor, act: torch.Tensor):\n",
        "        x = torch.cat([obs, act], dim=-1)\n",
        "        return self.q1(x)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# TD3 Agent\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class TD3Config:\n",
        "    gamma: float = 0.99\n",
        "    tau: float = 0.005\n",
        "    actor_lr: float = 1e-3\n",
        "    critic_lr: float = 1e-3\n",
        "\n",
        "    buffer_size: int = 500_000\n",
        "    batch_size: int = 256\n",
        "\n",
        "    # exploration (interaction) noise\n",
        "    exploration_noise: float = 0.1\n",
        "\n",
        "    # target policy smoothing noise\n",
        "    policy_noise: float = 0.2\n",
        "    noise_clip: float = 0.5\n",
        "\n",
        "    # delayed policy update\n",
        "    policy_delay: int = 2\n",
        "\n",
        "    # warmup steps of random actions\n",
        "    start_steps: int = 5_000\n",
        "\n",
        "    # how many gradient steps per environment step (can tune)\n",
        "    update_every: int = 1\n",
        "    gradient_steps: int = 1\n",
        "\n",
        "\n",
        "class TD3Agent(nn.Module):\n",
        "    def __init__(self, env, cfg: TD3Config = TD3Config(), device: str = \"cpu\"):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        # environment dims\n",
        "        self.discrete_act, self.discrete_obs, self.act_dim, self.obs_dim = rld.env_info(env)\n",
        "        assert not self.discrete_act and not self.discrete_obs, \"This TD3 assumes continuous actions and observations.\"\n",
        "\n",
        "        # action scaling (handles non-[-1,1] ranges)\n",
        "        self.act_limit = env.action_space.high.astype(np.float32)\n",
        "\n",
        "        # networks\n",
        "        self.actor = Actor(self.obs_dim, self.act_dim, self.act_limit).to(self.device)\n",
        "        self.actor_targ = Actor(self.obs_dim, self.act_dim, self.act_limit).to(self.device)\n",
        "        self.critic = CriticTwin(self.obs_dim, self.act_dim).to(self.device)\n",
        "        self.critic_targ = CriticTwin(self.obs_dim, self.act_dim).to(self.device)\n",
        "\n",
        "        self.actor_targ.load_state_dict(self.actor.state_dict())\n",
        "        self.critic_targ.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=self.cfg.actor_lr)\n",
        "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=self.cfg.critic_lr)\n",
        "\n",
        "        self.replay = ReplayBuffer(self.obs_dim, self.act_dim, size=self.cfg.buffer_size)\n",
        "\n",
        "        self.total_steps = 0\n",
        "        self.update_steps = 0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def select_action(self, obs: np.ndarray, explore: bool = True) -> np.ndarray:\n",
        "        self.total_steps += 1\n",
        "\n",
        "        # warmup: random actions\n",
        "        if explore and self.total_steps < self.cfg.start_steps:\n",
        "            return np.random.uniform(-self.act_limit, self.act_limit).astype(np.float32)\n",
        "\n",
        "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        act = self.actor(obs_t).cpu().numpy()[0]\n",
        "\n",
        "        if explore:\n",
        "            act += np.random.normal(0, self.cfg.exploration_noise, size=self.act_dim).astype(np.float32) * self.act_limit\n",
        "\n",
        "        return np.clip(act, -self.act_limit, self.act_limit).astype(np.float32)\n",
        "\n",
        "    def store(self, obs, act, rew, obs2, done):\n",
        "        self.replay.store(obs, act, np.float32(rew), obs2, np.float32(done))\n",
        "\n",
        "    def _soft_update(self, net, net_targ):\n",
        "        tau = self.cfg.tau\n",
        "        with torch.no_grad():\n",
        "            for p, p_targ in zip(net.parameters(), net_targ.parameters()):\n",
        "                p_targ.data.mul_(1 - tau)\n",
        "                p_targ.data.add_(tau * p.data)\n",
        "\n",
        "    def train_step(self):\n",
        "        if self.replay.size < max(self.cfg.batch_size, 1_000):\n",
        "            return\n",
        "\n",
        "        for _ in range(self.cfg.gradient_steps):\n",
        "            batch = self.replay.sample_batch(self.cfg.batch_size, self.device)\n",
        "            obs, act, rew, obs2, done = batch[\"obs\"], batch[\"act\"], batch[\"rew\"], batch[\"obs2\"], batch[\"done\"]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # target policy smoothing\n",
        "                noise = torch.randn_like(act) * self.cfg.policy_noise\n",
        "                noise = torch.clamp(noise, -self.cfg.noise_clip, self.cfg.noise_clip)\n",
        "                noise = noise * self.actor.act_limit  # scale by action limits\n",
        "\n",
        "                act2 = self.actor_targ(obs2)\n",
        "                act2 = torch.clamp(act2 + noise, -self.actor.act_limit, self.actor.act_limit)\n",
        "\n",
        "                q1_t, q2_t = self.critic_targ(obs2, act2)\n",
        "                q_t = torch.min(q1_t, q2_t)\n",
        "                backup = rew + self.cfg.gamma * (1.0 - done) * q_t\n",
        "\n",
        "            # critic update\n",
        "            q1, q2 = self.critic(obs, act)\n",
        "            critic_loss = F.mse_loss(q1, backup) + F.mse_loss(q2, backup)\n",
        "\n",
        "            self.critic_opt.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_opt.step()\n",
        "\n",
        "            # delayed actor update\n",
        "            if self.update_steps % self.cfg.policy_delay == 0:\n",
        "                actor_loss = -self.critic.q1_only(obs, self.actor(obs)).mean()\n",
        "\n",
        "                self.actor_opt.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_opt.step()\n",
        "\n",
        "                self._soft_update(self.actor, self.actor_targ)\n",
        "                self._soft_update(self.critic, self.critic_targ)\n",
        "\n",
        "            self.update_steps += 1\n",
        "\n",
        "    def episode(self, env, max_steps: int = 1000):\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        ep_ret = 0.0\n",
        "        ep_len = 0\n",
        "\n",
        "        while not done and ep_len < max_steps:\n",
        "            act = self.select_action(obs, explore=True)\n",
        "            obs2, rew, terminated, truncated, info = env.step(act)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            self.store(obs, act, rew, obs2, float(done))\n",
        "            obs = obs2\n",
        "\n",
        "            ep_ret += float(rew)\n",
        "            ep_len += 1\n",
        "\n",
        "            if self.total_steps % self.cfg.update_every == 0:\n",
        "                self.train_step()\n",
        "\n",
        "        info = dict(info) if isinstance(info, dict) else {}\n",
        "        info.setdefault(\"r_sum\", ep_ret)\n",
        "        info.setdefault(\"t\", ep_len)\n",
        "        return info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# initialise environment\n",
        "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
        "rld.seed_everything(42, env)\n",
        "\n",
        "# set up recording\n",
        "env = rld.Recorder(env, smoothing=10, video=True)\n",
        "recorder = env\n",
        "recorder.video = False\n",
        "tracker = rld.InfoTracker()\n",
        "\n",
        "# clip negative rewards (known trick in Walker environment)\n",
        "env = rld.transparent_wrapper(gym.wrappers.ClipReward)(env, min_reward=-10)\n",
        "\n",
        "# initialise TD3 agent (feel free to tune the config)\n",
        "cfg = TD3Config(\n",
        "    gamma=0.99,\n",
        "    tau=0.005,\n",
        "    actor_lr=3e-4,        # ↓ big one\n",
        "    critic_lr=1e-3,\n",
        "    buffer_size=500_000,\n",
        "    batch_size=256,\n",
        "    exploration_noise=0.08,  # slightly ↓\n",
        "    policy_noise=0.15,       # ↓\n",
        "    noise_clip=0.3,          # ↓\n",
        "    policy_delay=3,          # ↑ smoother actor updates\n",
        "    start_steps=5_000,\n",
        "    update_every=1,\n",
        "    gradient_steps=1,\n",
        ")\n",
        "agent = TD3Agent(env, cfg=cfg, device=\"cpu\")\n",
        "for n_epi in range(1000):\n",
        "\n",
        "    recorder.video = (n_epi + 1) % 100 == 0   # record video every 100 episodes\n",
        "    info = agent.episode(env)                 # collect one episode (+ train along the way)\n",
        "    env.add_stats(info, ignore_existing=True) # explicitly add stats in case of early termination\n",
        "    tracker.track(info)                       # track statistics\n",
        "\n",
        "    # update progress every 10 episodes\n",
        "    if (n_epi + 1) % 10 == 0:\n",
        "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
        "\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gymnasium",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
